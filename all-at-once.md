# Merging the two concepts
We have our low level blockchain, symbol registration and logical path layer, and we have the high level semantic data. We need to figure out how and where to leverage the lower to build the upper, and how the user data is handled, synched, replicated, etc.


# Low Level Primitives
I could imagine one of the early chains would define specifications for low level primitives that could be used and referenced in other projects (blockchain or not). The txn fee would probably be a crazy high amount of work. Something like days to weeks (maybe even a month?) of PoW to avoid unnecessary or useless primitives to be added (of course, someone still needs to implement it in software in some sort of client somewhere). Assuming we can use the [chain coordinates](https://github.com/ThinkingJoules/distributed-web-data/issues/2) we can encode these tags as a prefix on all the binary encodings so all data would be identifiable regardless of context. With sufficient work requirement you could easily end up with only a 2-3 byte prefix for all identifier tags. The goal is that this would become something like a permissionless and directly referenceable version of [multiformats](https://github.com/multiformats/multiformats) which is used in IPFS Content Identifiers.

# User Owned Data
The goal of all of these primtive symbols is to build some sort of 'Semantic Object Database'. This database should be able to describe data for ANY app/usecase. The database is itself could be thought of a labeling and encoding protocol. If the low level symbols have semantic understanding, then placing those symbols in a certain, well understood, pattern would give them meaning. If these sounds very similar to literal languages, then you are indeed following along. We are trying to create SYMBOLS + SYNTAX to allow both humans and computers to 'understand' the data. If we have well understood SYMBOLS + SYNTAX then interoperability will follow. 
## Nature of Unsiloing Data
When unsiloing data, the new API (between 'apps') becomes the query language of this database. The query language can even change really, because the underlying semantic symbols remain. So in order to unsilo, we MUST have a common understanding of what is what. This forces developers of applications to understand the semantics so when they are looking for data for concept X they know what query should return all of that data (and not miss some of it that meets requirements for X, but is not semantically understood/labeled as such). This sounds hard, but I think it is mostly different. And if this sounds like it will never work, then please propose fixes to these ideas or data will forever be siloed and centralized! I am only positing my best guess at a design. I believe composing (and understanding) API's from different centralized sources or understanding decentralized inputs to semantic labels is the tradeoff. It is a wholly new paradigm for developers, but once learned is complete in its design. You would have to learn new symbols as they are generated instead of entire APIs along with their own descriptors for new apps.
## Rough Topology
I do not want a global DHT of data. It does not allow fined grain control, and if users wanted to 'distribute' their data in that manner, it is still available. I just don't want it as the core means of retrieval. Since we want to allow private data, this also pushes back against a DHT at the core. DHT's are great when all the data is public for everyone, and that is why everyone has to share in the burden of replicating/persisting it. If it is private files, now you are just externalizing costs and burdening the system with no 'public' benefit.

So how does my hopeful design work?
### Validator 'Nodes' vs Data 'Nodes'
First some distinctions on the 'layers' to this topology.
#### Validator
So the 'nodes' used to run/validate the core chains would simply have a single key pair and use the pub key as its ID. This is pretty standard in distributed systems. This ID will accrue the work and 'gain' weight in the network over time. If the key is lost, you lose your accrued 'stake' and have to build it back up from zero using a new key/ID. I think these nodes can be discovered through bootstrap nodes, as is common in most blockchain designs.
#### Data
So user owned nodes would not necessarily have to participate in Validating. I think of these nodes as simply a computer that can answer queries to other data nodes about the data associated with a [User/Agent Identity](https://github.com/ThinkingJoules/distributed-web-data/blob/main/identifiers.md#distributed-pki) (let us use the term "**LLI**" (Long Lived Identifier) to mean an "Agent" for the rest of this document).

### IP Resolver
The goal is to build a robust network to handle updates to where to find the LLI. Here the LLI is basically a chain coorindate to the genesis transactions of a new identity. So it is basically two unsigned integers (with a 'context' integer for which chain these coordinates are for) that is equivalent to a **Domain**. I didn't say 'Domain *Name*' because 2 numbers generally aren't thought of as human friendly. However, the first billion or so identities would have fewer digits than a telephone number, and those are *memorizable*. So human friendly-ish. So like DNS we need a way to change the IP address associated with a LLI. We could add in 'human' names here, however we **MUST** allow collisions. The idea is that if a collision occurs, the LLI itself is needed to know for certain. I think the human names would really only be needed for discovery of or sharing of 'friends'. There is always the ability to extend the proof of work idea to allow ranking of collisions based on who expended more work to 'claim' the top search result.

I think this is purely a 'network' and not a chain. There is no real benefit to having a history of associated IPs. I am thinking something along the lines of [Fireflies](http://www.cs.cornell.edu/home/rvr/papers/Fireflies.pdf). It doesn't have to be this exact design, but I think it is compatible with all of the concepts of this new system.

# Recap so far
At this point in the design we have a way to identify a user (LLI) and resolve to an IP address to query it for data. This data is of a nature with 'universal' understanding and so can be queried to populate UI's for applications running locally on another users machine.

So things we still need to discuss:
* Permissions
* Repudiable vs Non-Repudiable Data
* Replication/Syncronization/Caching

# Permissions
This is going to be a hard one to solve simply. The finer grain control, the more effort for a user to set their permissions. The implementation of permissions is pretty flexible, in that it really only needs to query a set of three sets. You need to be in the intersection of a Group [of users], Capability, and [Group of] Resource in order to be able to perform the action indicated. The hardest part is making rules/ or adding new Resources to the proper set. If you had no rules, then it would push the complexity to the App to set the proper permissions on creation. This forces permissions to now be part of the larger data system and is no longer 'pretty flexible'. I think there are many ways of doing permissions, but trying to optimize for user experience and ease of understanding 'who' can 'see' 'what' ('when') is difficult. Hierarchical systems are nice for inheritance, but not all data is hierarchical. The right approach is yet to be determined. I suspect leveraging some sort of query language might help. That implies that, at least for permissions, ***a*** query language will be a dependency. This could/might differ from the query language used within applications, but I assume it will be one and the same.

# Repudiable vs Non-Repudiable Data
Now that we have a way to make data private, and solely in our control, we end up with a bit of a new problem. Can someone 'retract' data and deny they ever wrote it? Well, technically it is impossible in our system, since all things are authored with digital signatures. HOWEVER, in practice it can be very difficult to find and then disseminate evidence of a retraction. This goes back to our issue of [discoverability](https://github.com/ThinkingJoules/distributed-web-data/blob/main/identifiers.md#discoverability). 
## Use Case for Non-Repudiable Data
This is akin to a double spend if the data were a token. If we built a decentralized Twitter and someone tweets something that they later delete, they could claim they never tweeted it. If you cached a copy of it you can prove they did indeed author said tweet through digital signatures. However, what if you didn't have a cached copy? Maybe your server was offline temporarily? Maybe you were online, but the author selectively disseminatated the tweet to create confusion and get a group of people to think one thing, and another group to think another? You wouldn't even know that there was a tweet in which to scour the network for proof it actually exists. You would need to query everyone, all the time, to discover deliberate or accidental asymmetric data dissemination. This is only a problem for data that **MUST** be public, and instead is selectively shared. I think most social media data falls under this umbrella. I'm not advocating that things can never be filtered or marked as deleted, merely that having a log of it allows context, and even associated reasons why the content was deleted/edited.
### Grouping problem
As discussed in the [document](https://github.com/ThinkingJoules/distributed-web-data/blob/main/identifiers.md#blockchains-as-databases) about blockchains, we want to seperate state to only allow those who are interested to ***OPT-IN*** to tracking the state. Many people don't use Twitter, but use Facebook, or neither. If the distinction between apps fade, then the idea of a 'tweet' or 'post' cannot be assumed. This is a problem to still think about. It isn't clear how much or what type of data should be on any given subchain. If you put too much, then it will take a long time for late comers to replay all the state. 

In an ideal world you would have each subject ("tweet") have its own sub-subchain under the user namespace. But who validates these? How do you structure it in a way that the log is widely replicated to diverse actors so chain re-writes are detected? If you just let 'friends' validate then, it is prone to sybil like fake outs. I would prefer to keep this state separate from the PKI chain, so this would be a 'public statement' chain? The GLOBAL set of all public statements in a single chain? There are a lot of statements I don't care about or simply cannot read (foreign language). So how do we divide this up and ensure diverse actors?

This sounds like we WANT all the properties of a DHT like system, to programmatically assign data/chain to be tracked by a (random) other user. The idea being this user is unlikely to be controlled by the same author. However, this presents us with a new problem. We only have chain-level granularity. What if you get assigned a power user, when you yourself barely use the system? We need a way to distribute the data load fairly. Can we maybe combine some things here to get this to work?

[Explore DHT/Chain combo]







# Looking at Multiformats and CID
IPFS uses the aforementioned 'multiformats' to build their [Content Identifiers or CIDs](https://github.com/multiformats/cid). Is this new system going to have a CID or something similar? I think perhaps, but only for the identifiers for the 'values' of our triples. So I think ours would be quite different in content, but similar in concept. Given our [construction](https://github.com/ThinkingJoules/distributed-web-data/blob/main/semantic-objects.md#the-triple-20) of the 'value' we need to encode things such as FILE_EXT, UNIT, and LANG_TAG. So what about the hash? If we have a boolean 'true' value, do we hash that as well and add the tags? This seems bad to explode 1 bit of data to ~34 bytes (32 byte has + ~2 bytes for the FILE_EXT tag). Let us explore a different representation, where we move the concept of CID up to the concept of our 'value'.

# Describing the content of 'Value'
The only reason in our system to hash the data of value, is to provide de-duplication. This new system is not inherently content addressed. Hashing is nice to 'compress' the identity of large binary blob into a relatively small value. Since we are trying to treat 'primitive' datatypes as if they are files, we also want to treat very small files, as if they were 'primitives'. If we had a file that was only 16 bytes, would we really want to hash that? So ideally we want:
* Small values 'inlined' to prevent excess disk usage (and increase query performance)
* Large values hashed, and ideally chunked (to allow partial fetching. ie: streaming a video).

Do we need all of these, and/or how would this new system incorporate or get the effects of them? IPFS uses a single table in an attempt to save a byte for context namespace. I think it is best to break ours out to separate 'tables'(chains) and simply be slightly more inefficient, but the tradeoff is permissionless extensions and directly addressable/dereferencable definitions.

### Multiformat - Address
[These](https://github.com/multiformats/multiaddr/blob/master/protocols.csv) are URL-esque paths and schemes for addressing endpoints. I think it would be good to have a primitive equivalent to this. These symbols would allow multi-transport clients to route data as needed based on specific application use case. These chain coordinates can then be used as a universal alias for the 'scheme' portion of a URL.

### Multiformat - Base
[These](https://github.com/multiformats/multibase/blob/master/multibase.csv) would be useful for understanding a particular text encoding of binary data. I don't know that these are strictly necessary. If this new system is binary instead of text based, then these would really only be needed when transferring data on text based protocols. I personally feel it is up to the transport(address or scheme, from above) to specify encodings in their own way, as the text/binary dichotomy has always existed and is something for the protocol to determine. I think there could be a primitive 'Base' chain, but for the core system, it is not needed. It would mostly be for other systems to use these universal symbols to build their own version of a text based [IPFS CID]()

Not sure if all of 'multiformats' would be on a single chain or multiple. My default is more, simple chains. So looks like 3 chains according to their repo. [Addr](https://github.com/multiformats/multiaddr/blob/master/protocols.csv), [Base](https://github.com/multiformats/multibase/blob/master/multibase.csv), [Codec](https://github.com/multiformats/multicodec/blob/master/table.csv). 

Things I would consider for a 4th chain; Primitives: boolean, utf-8 blob, binary blob, var-int(u/i), fixed length integers (u8,u16...,i8,i16,...), floats of various flavors. Primitives would rarely be expanded, but I could see the early types going up to like u256, so if some app needed a u512, they could still add it. I think all IPFS CIDs are utf-8 that represent binary, and their 'Base' list is how to figure out how to get the right bits out of the string. My preference for the system is everything is binary by default. UI's can make things textual for us humans.

This could allow a new permissionless version of CIDs for use later in our non-DHT synching protocol.

# Extending Primitives
If you are making data machine readable, should you make it machine operatable? If we added another chain for [OPCODES](https://ethereum.org/en/developers/docs/evm/opcodes) we are well on our way to creating a universal (within this system) way to add functions to some sort of new machine semantic programming language. This seems extreme, but that is what is great by having this multi-chain system. We can just create the primitive chain first so we can have numbers and stuff, and then build on top of it later. That is the beauty of linked/semantic data. An interesting extension of this would be adding a suffix to primitives and can build conversion tables. Converting inches to feet or anything else the system has defined could be semantically understood and auto converted.
