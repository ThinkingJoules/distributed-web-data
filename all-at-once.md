# Merging the two concepts
We have our low level blockchain, symbol registration and logical path layer, and we have the high level semantic data. We need to figure out how and where to leverage the lower to build the upper, and how the user data is handled, synched, replicated, etc.


# Low Level Primitives
I could imagine one of the early chains would define specifications for low level primitives that could be used and referenced in other projects (blockchain or not). The txn fee would probably be a crazy high amount of work. Something like days to weeks (maybe even a month?) of PoW to avoid unnecessary or useless primitives to be added (of course, someone still needs to implement it in software in some sort of client somewhere). Assuming we can use the [chain coordinates](https://github.com/ThinkingJoules/distributed-web-data/issues/2) we can encode these tags as a prefix on all the binary encodings so all data would be identifiable regardless of context. With sufficient work requirement you could easily end up with only a 2-3 byte prefix for all identifier tags. The goal is that this would become something like a permissionless and directly referenceable version of [multiformats](https://github.com/multiformats/multiformats) which is used in IPFS Content Identifiers.

# User Owned Data
The goal of all of these primtive symbols is to build some sort of 'Semantic Object Database'. This database should be able to describe data for ANY app/usecase. The database is itself could be thought of as a labeling and encoding protocol. If the low level symbols have semantic understanding, then placing those symbols in a certain, well understood, pattern would give them meaning. If these sounds very similar to literal languages, then you are indeed following along. We are trying to create SYMBOLS + SYNTAX to allow both humans and computers to 'understand' the data. If we have well understood SYMBOLS + SYNTAX then interoperability will follow. 
## Nature of Unsiloing Data
When unsiloing data, the new API (between 'apps') becomes the query language of this database. The query language can even change really, because the underlying semantic symbols remain. So in order to unsilo, we MUST have a common understanding of what is what. This forces developers of applications to understand the semantics so when they are looking for data for concept X they know what query should return all of that data (and not miss some of it that meets requirements for X, but is not semantically understood/labeled as such). This sounds hard, but I think it is mostly different. And if this sounds like it will never work, then please propose fixes to these ideas or data will forever be siloed and centralized! I am only positing my best guess at a design. I believe composing (and understanding) API's from different centralized sources or understanding decentralized inputs to semantic labels is the tradeoff. It is a wholly new paradigm for developers, but once learned is complete in its design. You would have to learn new symbols as they are generated instead of entire APIs along with their own descriptors for new apps.
## Rough Topology
I do not want a global DHT of data. It does not allow fined grain control, and if users wanted to 'distribute' their data in that manner, it is still available. I just don't want it as the core means of retrieval. Since we want to allow private data, this also pushes back against a DHT at the core. DHT's are great when all the data is public for everyone, and that is why everyone has to share in the burden of replicating/persisting it. If it is private files, now you are just externalizing costs and burdening the system with no 'public' benefit.

So how does my hopeful design work?
### Validator 'Nodes' vs Data 'Nodes'
First some distinctions on the 'layers' to this topology.
#### Validator
The 'nodes' used to run/validate the core chains would simply have a single key pair and use the pub key as its ID. This is pretty standard in distributed systems. This ID will accrue the work and 'gain' weight in the network over time. If the key is lost, you lose your accrued 'stake' and have to build it back up from zero using a new key/ID. I think these nodes can be discovered through bootstrap nodes, as is common in most blockchain designs.
#### Data
User owned nodes would not necessarily have to participate in Validating. I think of these nodes as simply a computer that can answer queries to other data nodes about the data associated with a [User/Agent Identity](https://github.com/ThinkingJoules/distributed-web-data/blob/main/identifiers.md#distributed-pki) (let us use the term "**LLI**" (Long Lived Identifier) to mean an "Agent" for the rest of this document).

### IP Resolver
The goal is to build a robust network to handle updates to where to find the LLI. Here the LLI is basically a chain coorindate to the genesis transactions of a new identity. So it is basically two unsigned integers (with a 'context' integer for which chain these coordinates are for) that is equivalent to a **Domain**. I didn't say 'Domain *Name*' because 2 numbers generally aren't thought of as human friendly. However, the first billion or so identities would have fewer digits than a telephone number, and those are *memorizable*. So human friendly-ish. So like DNS we need a way to change the IP address associated with a LLI. We could add in 'human' names here, however we **MUST** allow collisions. The idea is that if a collision occurs, the LLI itself is needed to know for certain. I think the human names would really only be needed for discovery of or sharing of 'friends'. There is always the ability to extend the proof of work idea to allow ranking of collisions based on who expended more work to 'claim' the top search result.

I think this is purely a 'network' and not a chain. There is no real benefit to having a history of associated IPs. I am thinking something along the lines of [Fireflies](http://www.cs.cornell.edu/home/rvr/papers/Fireflies.pdf). It doesn't have to be this exact design, but I think it is compatible with all of the concepts of this new system.

# Recap so far
At this point in the design we have a way to identify a user (LLI) and resolve to an IP address to query it for data. This data is of a nature with 'universal' understanding and so can be queried to populate UI's for applications running locally on another users machine.

So things we still need to discuss:
* Permissions
* Repudiable vs Non-Repudiable Data
* Authorship
* Replication/Syncronization/Caching

# Permissions
This is going to be a hard one to solve simply. The finer grain control, the more effort for a user to set their permissions. The implementation of permissions is pretty flexible, in that it really only needs to query a set of three sets. You need to be in the intersection of a Group [of users], Capability, and [Group of] Resource in order to be able to perform the action indicated. The hardest part is making rules/ or adding new Resources to the proper set. If you had no rules, then it would push the complexity to the App to set the proper permissions on creation. This forces permissions to now be part of the larger data system and is no longer 'pretty flexible'. I think there are many ways of doing permissions, but trying to optimize for user experience and ease of understanding 'who' can 'see' 'what' ('when') is difficult. Hierarchical systems are nice for inheritance, but not all data is hierarchical. The right approach is yet to be determined. I suspect leveraging some sort of query language might help. That implies that, at least for permissions, ***a*** query language will be a dependency. This could/might differ from the query language used within applications, but I assume it will be one and the same.

# Repudiable vs Non-Repudiable Data
Now that we have a way to make data private, and solely in our control, we end up with a bit of a new problem. Can someone 'retract' data and deny they ever wrote it? Well, technically it is impossible in our system, since all things are authored with digital signatures. HOWEVER, in practice it can be very difficult to find and then disseminate evidence of a retraction. This goes back to our issue of [discoverability](https://github.com/ThinkingJoules/distributed-web-data/blob/main/identifiers.md#discoverability). 
## Use Case for Non-Repudiable Data
This is akin to a double spend if the data were a token. If we built a decentralized Twitter and someone tweets something that they later delete, they could claim they never tweeted it. If you cached a copy of it you can prove they did indeed author said tweet through digital signatures. However, what if you didn't have a cached copy? Maybe your server was offline temporarily? Maybe you were online, but the author selectively disseminatated the tweet to create confusion and get a group of people to think one thing, and another group to think another? You wouldn't even know that there was a tweet in which to scour the network for proof it actually exists. You would need to query everyone, all the time, to discover deliberate or accidental asymmetric data dissemination. This is only a problem for data that **MUST** be public, and instead is selectively shared. I think most social media data falls under this umbrella. I'm not advocating that things can never be filtered or marked as deleted, merely that having a log of it allows context, and even associated reasons why the content was deleted/edited.
### Grouping problem
As discussed in the [document](https://github.com/ThinkingJoules/distributed-web-data/blob/main/identifiers.md#blockchains-as-databases) about blockchains, we want to separate state to only allow those who are interested to ***OPT-IN*** to tracking/validating the state. Many people don't use Twitter, but use Facebook, or neither. If the distinction between apps fade, then the idea of a 'tweet' or 'post' cannot be assumed. It isn't clear how much or what type of data should be on any given subchain. If you put too much, then it will take a long time for late-comers to replay all the state. 

In an ideal world you would have each subject ("tweet") have its own sub-subchain under the user namespace. But who validates these? How do you structure it in a way that the log is widely replicated to diverse actors so chain re-writes are detected? If you just let 'friends' validate then, it is prone to sybil like fake outs. I would prefer to keep this state separate from the PKI chain, so this would be a 'public statement' chain? The GLOBAL set of all public statements in a single chain? There are a lot of statements I don't care about or simply cannot read (foreign language). So how do we divide this up and ensure diverse actors?

So let us think about WHY we are trying to use a chain in this scenario.
1) We need diverse replication/validation
2) We need to easily detect (or simply disallow) deletion of content

#1 is really just a way to ensure #2. But validation of this type of data is really simple. It is really a schema validator like our low level validators. We don't care what they tweet, just so long as it follows schema rules, and putting it on a chain gives you historical values. But do we need the actual ***content*** to be on-chain? I would argue **no**. 
#### Idea #1 Detached Content
If we know tweet X was created with some state represented by hash Y. Validators simply need to verify that the binary data associated with Y is valid to accept the state. Validators need to ***see*** all the state transitions to know it's valid, but for people trying to know a previous state existed, the hash is sufficient. This is similar to what Bitcoin did with [Segregated Witness](https://cointelegraph.com/explained/segwit-explained). Now if someone publishes state, the parties that did not get the content disseminated to them ***knows*** they are missing state, they just need to find the data that matches the hash. How would they find this state? First, try asking the author. If they refuse your request, then you need to find it else where. What are your options? Ask all your 'friends'. If they don't know the hash then it gets much harder. The only way to get the unknown state is to have a DHT or some other parallel system to publish content, and **hope** that someone who has seen it, publishes it there. This other network would basically act like any torrent service, and suffer from all the same shortcomiongs. If ***no one*** in the network stores the content of a hash, then it is lost forever. The base case is that you would, at a minimum, store your own transitions and respond to queries. Basically, refusing requests is like Blocking someone on Twitter. You can see their tweet if someone non-blocked shares a picture of it with you. This illustrates how this approach is compatible with the *idea* of permissions. This solution still doesn't solve the grouping problem.
#### Idea #2 Ethereum Like Rollups
I'm not well versed in all the Ethereum stuff, but I know their scaling tech is pretty neat. Ethereum has various forms of [Rollup](https://medium.com/fcats-blockchain-incubator/how-zk-rollups-work-8ac4d7155b0e) constructions that are trying to solve a similar problem to us. However, since they are dealing specifically with tracking transfers, I'm not really sure how it would translate to our usecase. We really don't want the Zero Knowledge part, because for our particular public data usecase, we don't want the privacy. I'm not sure if these solutions could be adapted, but I wanted to just put it here for discussion.

### Grouping thoughts cont.
So we have a way to minimize the total state of any chain through detaching the content, but how many chains do we need? One for each person? One monolith?

I think the best way to look at it is in the nature of format. If we **must** have a chain to get certain guarantees, then perhaps we can leverage the properties of a chain for our own advantage. If the state transition is simply a binary [delta encoding](https://en.wikipedia.org/wiki/Delta_encoding), then edits can be performed to the 'target'. This would allow git-like operations to any 'value'.

In our data triple we have a Subject, Property, and Value. The Property has all the constraints, semantic meaning, and context for the Value. So what if we have chains based on Properties? That way a logical object, may have data that is repudiable and non-repudiable with a unifying Subject identifier? So PropDef would have a property like ```non-repudiable``` that is a boolean. Since the property also carries the FILE_EXT as well as any constraints, it can allow each of these chains to have encoding-specific validation of the binary after a new delta encoding has been submitted. If it fails, the 'transaction' is reject by the validators of this PropertyValue Chain. To reference a version, a Subject with any non-repudiable Property would simply have the chain coordinates as its value. These would be pointing to the last delta transaction for the document. Depending on the use-case these PropertyValue chains might simply have a ```parent``` field to allow any author to 'build' off of any other logical document by any other author. I could see particular use-cases where that is not allowed, and that is why having separate chains for each property allows flexibility (at the cost of initial configuration, but through self-similarity tooling can lower these burdens).

It is less clear who might validate these chains. They are sort of 'once removed' from the core chains, but are relevant to many different people. I would assume at first, validators might validate several chains, splitting their work. However, we could make the work re-usable across chains? This would make attacking the whole system easier, instead of making attacking any one chain easier. If an attack can, at worst cause the chain to stop (DDoS) then these non-repudiable chains would be most targeted since it would be the most disruptive to the larger use of the system. However, because the users are more diverse and there are simply more of them, it would be in their interest to put some sort of validation effort in, to combat against a single entity easily gaining >30% of the total 'staked' weight on that chain. This would mean, if you use this property in your data, you are at least keeping the transaction state, and can 'forget' the observed delta binaries you have validated. This will save disk space, but still require CPU and Bandwidth.

## Non-Repudiable Data Summary
It is difficult to get the right balance. We don't want a monolithic chain like Ethereum, but once you start having more than one chain, it is hard to find clean a simple distinctions. I'm not wholly satisfied with my constructions above. I suppose we don't need to determine at a low level what is non-repudiable, and let early apps build their own sub-chains with perhaps all non-repdudiable data for that particular app on a single chain. Then see how it evolves from there. If they are going to reference chain coordinates as the 'value' in the triple then that is all I proposed. The difference is that everything is repudiable in the base system, and we can 'key' up what an LLI has authored on various non-repudiable sub-chains, but in an ad-hoc instead of methodical manner.

## Repudiable Data
Otherwise, data as we mostly know it currently on the internet. I think this is pretty straightforward. I would propose using a [CRDT](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type) like GunDB to do data merges and allow offline editing. This data is what will be mostly used as the 'value' in the triple. Anything that is private by default would be here. Getting the two different data types to work together might be tricky, but there is no way (that I can see) how to get non-repudiation easily. They are simply different paradigms. The best we can do, is make sure that the non-repudiable data can ***fit*** in to the rest of the triple pattern so it *looks* like repudiable data and can then integrate into the larger indexing and querying systems that will be built around that.

# Data Authorship
So verifying an author in non-repudiable data is pretty easy, since you have to sign at each state transition. However for doing repudiable data we have two options. Sign every state on every change, or sign a root of a merkle tree containing all the data. 
## Sign all the things
This one is straight forward and easy to reason about. Any keys that are part of you LLI can sign at any time and publish the data widely (and wildly) and the CRDT will deal with it at ANY point in the network. Pros: Very flexible at the network level. Cons: LOTS of signatures for the entire system to validate (CPU). This is the approach GunDB takes. Everything is signed on your private graph and the network validates all signatures. The data itself is self-proving (With the LLI scheme, you would still need to see if the particular key they signed with is currently valid, so it is really self-prov**able**).
## Sign Once, share proofs
This one probably fits with the topology of the network just fine as I've described it so far. This requires the CRDT to only happen on data amongst your own private servers. One of them creates a root by hashing all branches of the trie that changed since last 'commit'. Once fully hashed, you sign the root once. You can only 'prove' data that is part of that root. Data within the tree has no way of knowing if it is authentic without asking the LLI endpoint for a proof. The upside, this proof can verify N pieces of data. It is a bit more bandwidth (maybe not, signatures are long) but you can perform 1000's of hashes before you can do a single asymmetric key verification. This proof could potentially become zero-knowledge. This would slow things down some, but would save on bandwidth, and not reveal sibling nodes in your trie. Not sure what the break-even would be for Zk vs conventional merkle proof.
## Tradeoffs
Signing everything allows public data to be widely distributed and cached anywhere. However, if you want non-repudiable public data you can't use this method at all, and I think most fully-public data is going to want this quality. So I think we can take that out of consideration. The closest you could get is like widely shared, semi-public data (think large organization/institution). In that setting it could help network and app performance. However, even being in a small business I know I would want audit logs, and hence, we are back to non-repudiable. If you trust your own software, and your sys admins, then you could store data in a log like manner in your repudiable data store. It really depends on your trust profile. This brings up a good point though. If we only have a single signature at the root, but we allow other authors for subsets of the data, do we not still have to store their sig *within* our trie? Or do we just say that they edited it but throw out the sig, since the main server (the one creating the merkle trie) always has more authority and is trusted as the arbiter of truth in this federated namespace? If we want to know *who* last edited it, we would need need to either build a repudiable log of changes or we need a spot for this info alongside the CRDT metadata. Putting it in the CRDT metadata makes the most sense as you only care about the *last* edit. You can still build a repudiable log, but now don't have to record the editor manually.
## Futureproofing
Quantum proof signatures are big and ugly and expensive. It is when not if, and I don't foresee quantum proof signature being very performant when the need arises.
## Authorship Summary
Given the following:
* The proposed network topology (data is always queried at source)
* Fully public data is almost always going to want to have the property of non-repudiation
* Quantum Proof signatures suck

I propose the signed root be used as the current design for repudiable data authorship.

# Replication/Syncronization/Caching
I envision an owner of an LLI basically having a couple servers, one of which is 'public' that answers the queries. Tooling would be needed to help simplify the configuration and orchestration between these. One trick could be to add the locally generated key on these servers to your LLI/PKI. Then all servers 'know' who they belong to and who it is safe to share private data with. There is still going to need to be some amount of configuration, depending on the device, amount of storage, etc.

Regardless of the authorship structure, I think the data will probably end up in a merklized PATRICIA trie for at minimum the Syncronization attribute.
## Replication
As discussed before, you might replicate data for your friends, so in the event of data loss, they can recover files. I will go in to more details later on what I mean as 'files'. I expect the files will be encrypted before sending, and will be opaque. The only thing you know, is that you are storing this for only the people you have selected. You cannot answer queries on your friends behalf or know the contents. Perhaps this can change, but being able to answer queries makes you now need to know ALL of their permissions. This is perhaps where the magic zero knowledge stuff comes in, but to start with, I am going to keep this simple.
## Synchronization
This is mostly between your own devices. The key issue is repudiable data (basically key-value data) is hard to syncronization without asking for all the keys. Using a merklized prefix trie would allow you to probe the other servers and then sync a set of branches. Since the merklized patricia trie is effectively a copy on write structure the online node could track commit a root when the other goes offline, and when it comes back it can even know in advance exactly what updates to send the now back online node. There would still be a level of diffing, as the offline node might have new offline changes to send the other direction as well. Either way, having a trie makes it much easier to reason about how to potentially save a lot of time on restarts
## Caching
If you are running the app locally and building a view, it would be nice to cache all your friends data changes constantly in the background. Then you can simply query your local database in their namespace to save time and avoid network calls. Your server could always send out another query to ensure the data is accurate. If it isn't, then as long as you can push new changes to the app view (through websockets if in the browser) then the view will update with data as it comes in (GunDB used this pattern extensively and works really well). Having a prefix trie allows you to query or cache an exact match of your friends data to allow really fast view. The synchronization technique could also apply to a friends servers if you are trying to hold a whole subtree of their data locally.

# Describing the content of 'Value' in the triple
The only reason in our system to hash the data of the value, is to provide de-duplication. This new system is not inherently content addressed. Hashing is nice to 'compress' the identity of large binary blob into a relatively small value. Since we are trying to treat 'primitive' datatypes as if they are files, we also want to treat very small files, as if they were 'primitives'. If we had a file that was only 16 bytes, would we really want to hash that? So ideally we want:
* Small values 'inlined' to prevent excess disk usage (and increase query performance)
* Large values hashed, and ideally chunked (to allow partial fetching. ie: streaming a video).

If we also go the single signature PATRICIA trie route, we need to include whatever bytes are at the 'value' in our proof. (A PATRICIA node is basically a hash of (Subject + Property + Value)). So if we have large values they will increase our proof size. If we are optimizing for proof size, then we want to inline anything less than our hash length and hash anything longer than our hash length.


--- WIP ---






# Looking at Multiformats and CID
IPFS uses the aforementioned 'multiformats' to build their [Content Identifiers or CIDs](https://github.com/multiformats/cid). Is this new system going to have a CID or something similar? I think perhaps, but only for the identifiers for the 'values' of our triples. So I think ours would be quite different in content, but similar in concept. Given our [construction](https://github.com/ThinkingJoules/distributed-web-data/blob/main/semantic-objects.md#the-triple-20) of the 'value' we need to encode things such as FILE_EXT, UNIT, and LANG_TAG. So what about the hash? If we have a boolean 'true' value, do we hash that as well and add the tags? This seems bad to explode 1 bit of data to ~34 bytes (32 byte has + ~2 bytes for the FILE_EXT tag). Let us explore a different representation, where we move the concept of CID up to the concept of our 'value'.


Do we need all of these, and/or how would this new system incorporate or get the effects of them? IPFS uses a single table in an attempt to save a byte for context namespace. I think it is best to break ours out to separate 'tables'(chains) and simply be slightly more inefficient, but the tradeoff is permissionless extensions and directly addressable/dereferencable definitions.

### Multiformat - Address
[These](https://github.com/multiformats/multiaddr/blob/master/protocols.csv) are URL-esque paths and schemes for addressing endpoints. I think it would be good to have a primitive equivalent to this. These symbols would allow multi-transport clients to route data as needed based on specific application use case. These chain coordinates can then be used as a universal alias for the 'scheme' portion of a URL.

### Multiformat - Base
[These](https://github.com/multiformats/multibase/blob/master/multibase.csv) would be useful for understanding a particular text encoding of binary data. I don't know that these are strictly necessary. If this new system is binary instead of text based, then these would really only be needed when transferring data on text based protocols. I personally feel it is up to the transport(address or scheme, from above) to specify encodings in their own way, as the text/binary dichotomy has always existed and is something for the protocol to determine. I think there could be a primitive 'Base' chain, but for the core system, it is not needed. It would mostly be for other systems to use these universal symbols to build their own version of a text based [IPFS CID]()

Not sure if all of 'multiformats' would be on a single chain or multiple. My default is more, simple chains. So looks like 3 chains according to their repo. [Addr](https://github.com/multiformats/multiaddr/blob/master/protocols.csv), [Base](https://github.com/multiformats/multibase/blob/master/multibase.csv), [Codec](https://github.com/multiformats/multicodec/blob/master/table.csv). 

Things I would consider for a 4th chain; Primitives: boolean, utf-8 blob, binary blob, var-int(u/i), fixed length integers (u8,u16...,i8,i16,...), floats of various flavors. Primitives would rarely be expanded, but I could see the early types going up to like u256, so if some app needed a u512, they could still add it. I think all IPFS CIDs are utf-8 that represent binary, and their 'Base' list is how to figure out how to get the right bits out of the string. My preference for the system is everything is binary by default. UI's can make things textual for us humans.

This could allow a new permissionless version of CIDs for use later in our non-DHT synching protocol.

# Extending Primitives
If you are making data machine readable, should you make it machine operatable? If we added another chain for [OPCODES](https://ethereum.org/en/developers/docs/evm/opcodes) we are well on our way to creating a universal (within this system) way to add functions to some sort of new machine semantic programming language. This seems extreme, but that is what is great by having this multi-chain system. We can just create the primitive chain first so we can have numbers and stuff, and then build on top of it later. That is the beauty of linked/semantic data. An interesting extension of this would be adding a suffix to primitives and can build conversion tables. Converting inches to feet or anything else the system has defined could be semantically understood and auto converted.
