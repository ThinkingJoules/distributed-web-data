# The Current Web
For the most part humans deal with legible namespaces with non-colliding names. This is great for humans but creates scarcity as if they are tokens (Domain names are just non-blockchain digital NFTs). Sure, you can use the web with just an IP address, but most technology needs a static IP to be robust. DNS can solve for dynamic IP addresses, but either way, Domains and IP addresses are scarce resources and therefore are hard to acquire for the average internet citizen. If you cannot secure a  foundational namespace because it is not [accesible](https://github.com/ThinkingJoules/distributed-web-data/wiki/Permissionless-vs-Accessible) then centralization ensues. The goal is to avoid centralization and so it is important to understand the underlying drivers of it. 

# The Semantic Web
While I was reading the ["A Developer's Guide to the Semantic Web"](https://www.amazon.com/Developers-Guide-Semantic-Web/dp/3662437953) I couldn't help but notice the implicit dependency on DNS for the system to work. I know they are URI's and don't have to resolve, but you still need a namespace. This is even more true in [Atomic Data](https://docs.atomicdata.dev/atomic-data-overview.html) where URI's are URL's and ***should*** always resolve. Perhaps I'm misunderstanding the RDF spec, but making them resolvable is the only way it really makes sense to me, otherwise how do we know/learn about the namespace's identifiers? The book I linked above is nearly 800 pages. So perhaps the spec is perfect in every way, but it is not very accessible to me. And if something is hard to reason about, then reasoning about how to build something new using it, is ~~exponentially~~ infinitely harder. There are many other critiques of RDF and I don't want to get in to them here. **The goal of RDF is something I agree with**. I just don't think the execution of the goal is scalable to get enough developers to understand it well enough to be productive in it.

# Discoverability
Currently the web is vast and is only loosely connected by open protocols such as IP and HTTP. Discovery or finding relevant sites/data is next to impossible. Currently it is a monstrous effort to crawl the web and generate indicies and tags and algorithims to make it easy for us to find stuff. Google is the obvious example. Searching the web is really the only way the current web is usable. The ideas of the Semantic web (should) make tagging and indexing pretty much free. This still requires people crawling the entire web to learn about information. Removing the two hardest parts of building a search engine should make it much easier to build discovery mechansims.
## Reuse through Discoverability - One Step Farther
Another thing I didn't like about RDF is that there is no mechanism to even attempt to find other semantic definitions to try and reuse. If you didn't know about schema.org or FOAF then how would you know to try and reuse those first? And you probably found those through... Google. In my opinion, this adds to the uselessness of RDF. So now we need to create equivalency of semantics after the fact?? That seems insane. Where is the reuse? To me, it feels similar to attempting to avoid the double spend problem while letting the data be created in an unorganized manner (think if you needed Google to index the entire web to make sure two Bitcoin txns didn't double spend). Basically to solve the double spend problem (which is really DISCOVERING the violation of certain rules) blockchains makes a central place for all rule-following state to be created. You can't create invalid state in blockchain land (at worst there is a hard fork).

Now we can't reject semantic equivalency programatically in a VM, since this is all about human meaning. However, a blockchain basically removes the need to crawl the web to index and tag data. So by putting all semantic definitions on a chain, it is trivial to create a search engine (block explorer in blockchain terms) to find all semantic definitions that might already be created. This feels way easier to build then building a Google like search engine for definitions.

To be clear, I'm not advocating ALL data go on chain. Only foundational definitions, such as Classes, Properties, DataTypes, etc. The stuff that developers might need to work with and potentially create new ones. End users should be using these definitions in off-chain data.

# Blockchains as databases
The problem with [blockchains](https://github.com/ThinkingJoules/distributed-web-data/wiki/Blockchains), is that everything on a single chain must be communicated to every participant. Avalanche and Polkadot solve this problem with creating 'subnets' and 'parachains' respectively. The main idea is to not encumber validators in state they don't care about. 

Using this pattern would allow people who perhaps only care about certain aspects of the Digital Reality to contribute resources only to the things they care about. Perhaps they really care about the Property Definition Chain and can run it easily on a Raspberry Pi. Whereas a monolithic single chain, such as Ethereum, every node needs to be massive to deal with the communication/computational/storage requirements. A normal blockchain requires a token for security, but I think I have come up with a design that is permissionless, protected from both Sybil and Spam protection, and **does not** require the acquisition of a token to use. Read about the design of what I call the [Tokenless Blockchain or TChain for short](https://github.com/ThinkingJoules/linked-data-thoughts/wiki/'Tokenless'-Proof-of-Stake-Blockchain-(PoSW-=-Proof-of-Staked-Work))(given the constraint there is no transferring of a token).

# Chain of Chains
If all of these Tchains were registered on some top level Tchain (sort of like the Platform-Chain in Avalanche) then that would let everyone know what chains exist and other info about them. A concrete example of an extremely simple subchain: you could have each block in the chain simply be a single transaction (given the chain is low velocity) and the payload simply be a JSON object that follows the schema rules for that chain (VM is really just a schema validator). Each block, in essence becomes just a row in a table. Obviously the VM can be as complex or simple as you want it. I'm just trying to illustrate how simple a subchain as database could be. 

Would need to think about all the various ways a chain might be configured and what requirements the registration chain might have. (Side thought: In theory, there might be a use case where a particular subchain VM might have it's own form of registration for sub-subchains. I suppose if it made sense for their use case, it is perfectly fine to do that. They are just not directly discoverable/indexed from the main registration chain.)

### Namespace and addressability
If there is a single registration point, all subchains can be in relation to them. They could have their own sort of URL convention: 

```dr://[Registration TxnID for Subnet]/[subnet specific syntax]```

I would put the URL path in order of high level to low level (opposite of a DNS URL that has the TLD at the end). The subchain could be thought of as a TLD, but for prefix reasons, I would probably move it's position for this URL usecase. I think the ```dr``` scheme would handle a default way to negotiate transport protocols to allow http, websockets, quic, etc. I would assume http would be the default as it is the oldest, most well known, and has the most tooling built around it. We could put http:// first, but then it is hard to disambiguate when NOT to use DNS to resolve (the path part of the URL probably isn't DNS compliant. I feel there needs to be some sort of additional prefix in the URL spec to make it less of a requirement to always alter the Scheme even if it still uses HTTP).

Instead of TxnID (hash) could we use two unsigned integers? Like (BlockNumber,TxnNumberInBlock)? It would be way better than hashes for compactness. [More discussion on that here](https://github.com/ThinkingJoules/distributed-web-data/issues/2)

## Distributed PKI
My personal goal is building this aspect of the Digital Reality. I would really like a static identifier that remains unchanged between key rotations. Allowing more than one key per logical identity could also allow ranking of keys to allow devices to sign stuff for other applications, for example, but not be able to add more keys to the identity.

Creating a long lived identifier (LLI) could allow any application to key information to a specific user, and allow cross-ecosystem verification of authenticity of data.

# Summary
I think tokenless blockchains for core semantic symbol registrations makes it easier for me to both reason about how to build something complex, and, more critically, how tooling could be pretty trivally built for discovering and reusing primitives. I can play out in my head how much of the tooling might look or work using something as described. I can also see how it is extensible for future expansion. I'm afraid non-chain solutions are just pushing design complexity down the road. I don't want to rebuild Google or crawl the web, SPECIFICALLY for foundational, core, information that is used to actually define the system. The [reification](https://en.wikipedia.org/wiki/Reification_(computer_science)#On_Semantic_Web) process doesn't have to be in the same format to still serve the same goal (reading through these chains, the current state can be stored (compiled) in triples). I feel the recursive subchain structure is a more explicit understanding of what we are reifiying and the context in which we are working.

Given a chain nature, we could also find updates or extensions to data if we added a sort of 'previousVersion' field, and point back to the old one. It would probably be better to use patch updates, so an 'updates' field points to the sort of chain within the chain to update on. If 'updates' is blank, then this is the creation of a new symbol. I should create some pics/diagrams/examples to make it clearer for those less familiar with append only logs.
